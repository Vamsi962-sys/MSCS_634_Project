{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe0bf11",
   "metadata": {},
   "source": [
    "# MSCS 634 – Final Project (Deliverable 4)<br>\n",
    "### Consolidated Analysis: Online Retail II (2009–2010)<br>\n",
    "**Course:** MSCS-634 Advanced Big Data and Data Mining<br>\n",
    "**Student:** Vamsi Krishna Gajulapalli<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a623ac",
   "metadata": {},
   "source": [
    "## 1. Introduction<br>\n",
    "This project analyzes the **Online Retail II** transaction dataset to demonstrate an end-to-end data mining workflow. The goal is to convert raw transactional data into practical business insights using:<br>\n",
    "• **Data Cleaning & EDA** (understanding patterns and data quality)<br>\n",
    "• **Regression** (predicting transaction value)<br>\n",
    "• **Classification** (identifying high-value customers)<br>\n",
    "• **Clustering** (customer segmentation)<br>\n",
    "• **Association Rule Mining** (frequently co-purchased products)<br><br>\n",
    "The final output includes clear visualizations, model evaluation metrics, and recommendations aligned with real-world retail decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e102fd9",
   "metadata": {},
   "source": [
    "## 2. Dataset Description<br>\n",
    "**Dataset:** Online Retail II (Excel)<br>\n",
    "**Why chosen:** It is a realistic, high-volume retail dataset with product, customer, and time-based fields—ideal for regression, classification, clustering, and pattern mining.<br><br>\n",
    "**Key columns:** Invoice, StockCode, Description, Quantity, InvoiceDate, Price/UnitPrice, CustomerID, Country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed91a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup (run once)\n",
    "# ============================================================\n",
    "import sys\n",
    "\n",
    "# Install packages only if missing (safe to re-run)\n",
    "!{sys.executable} -m pip install -q pandas numpy matplotlib seaborn openpyxl scikit-learn mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d2008",
   "metadata": {},
   "source": [
    "## 3. Data Loading, Cleaning, and Preparation<br>\n",
    "The raw dataset contains missing values (CustomerID/Description), duplicate rows, returns (negative Quantity), and invalid pricing. Cleaning ensures we model **valid purchase behavior** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c999f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3.1: Load dataset\n",
    "# ============================================================\n",
    "df_raw = pd.read_excel(\"online_retail_II.xlsx\")\n",
    "\n",
    "print(\"Raw dataset shape (rows, cols):\", df_raw.shape)\n",
    "display(df_raw.head())\n",
    "\n",
    "# ============================================================\n",
    "# Step 3.2: Standardize column names\n",
    "# ============================================================\n",
    "df_raw.columns = df_raw.columns.str.strip().str.replace(\" \", \"\")\n",
    "\n",
    "# Some dataset versions use 'Price' instead of 'UnitPrice'\n",
    "if \"UnitPrice\" not in df_raw.columns and \"Price\" in df_raw.columns:\n",
    "    df_raw = df_raw.rename(columns={\"Price\": \"UnitPrice\"})\n",
    "\n",
    "# ============================================================\n",
    "# Step 3.3: Clean dataset\n",
    "# - drop missing CustomerID/Description\n",
    "# - remove duplicates\n",
    "# - remove returns (Quantity <= 0)\n",
    "# - remove invalid pricing (UnitPrice <= 0)\n",
    "# - ensure InvoiceDate is datetime\n",
    "# ============================================================\n",
    "df_clean = df_raw.dropna(subset=[\"CustomerID\", \"Description\"]).drop_duplicates()\n",
    "df_clean = df_clean[(df_clean[\"Quantity\"] > 0) & (df_clean[\"UnitPrice\"] > 0)]\n",
    "\n",
    "df_clean[\"InvoiceDate\"] = pd.to_datetime(df_clean[\"InvoiceDate\"], errors=\"coerce\")\n",
    "df_clean = df_clean.dropna(subset=[\"InvoiceDate\"])\n",
    "\n",
    "print(\"Cleaned dataset shape:\", df_clean.shape)\n",
    "display(df_clean.head())\n",
    "\n",
    "# ============================================================\n",
    "# Step 3.4: Feature: transaction revenue\n",
    "# ============================================================\n",
    "df_clean[\"TotalPrice\"] = df_clean[\"Quantity\"] * df_clean[\"UnitPrice\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2dd7f",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)<br>\n",
    "EDA helps validate assumptions and reveals patterns such as skewed revenue distribution, sales concentration by country, and seasonality across months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed24753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EDA 4.1: Quantity distribution\n",
    "# ============================================================\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.histplot(df_clean[\"Quantity\"], bins=50)\n",
    "plt.title(\"Distribution of Quantity\")\n",
    "plt.xlabel(\"Quantity\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# EDA 4.2: Log(1 + TotalPrice) distribution (reduces skew)\n",
    "# ============================================================\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.histplot(np.log1p(df_clean[\"TotalPrice\"]), bins=50)\n",
    "plt.title(\"Log(1 + TotalPrice) Distribution\")\n",
    "plt.xlabel(\"Log(1 + TotalPrice)\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# EDA 4.3: Top 10 countries by total sales\n",
    "# ============================================================\n",
    "country_sales = df_clean.groupby(\"Country\")[\"TotalPrice\"].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "country_sales.plot(kind=\"bar\")\n",
    "plt.title(\"Top 10 Countries by Total Sales\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# EDA 4.4: Monthly sales trend (seasonality)\n",
    "# ============================================================\n",
    "df_clean[\"Month\"] = df_clean[\"InvoiceDate\"].dt.month\n",
    "monthly_sales = df_clean.groupby(\"Month\")[\"TotalPrice\"].sum().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "monthly_sales.plot()\n",
    "plt.title(\"Monthly Sales Trend\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# EDA 4.5: Correlation among numeric transaction features\n",
    "# ============================================================\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(df_clean[[\"Quantity\",\"UnitPrice\",\"TotalPrice\"]].corr(), annot=True, fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix (Transaction-Level)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba52e20",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering<br>\n",
    "We add time-based features and a log-transformed target for stable learning. We also add invoice-level behavioral features to represent basket diversity and basket size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5.1: Time-based features\n",
    "# ============================================================\n",
    "df_clean[\"Year\"] = df_clean[\"InvoiceDate\"].dt.year\n",
    "df_clean[\"DayOfWeek\"] = df_clean[\"InvoiceDate\"].dt.dayofweek  # Monday=0\n",
    "df_clean[\"Hour\"] = df_clean[\"InvoiceDate\"].dt.hour\n",
    "\n",
    "# Log transform of revenue (target for regression)\n",
    "df_clean[\"LogTotalPrice\"] = np.log1p(df_clean[\"TotalPrice\"])\n",
    "\n",
    "# ============================================================\n",
    "# Step 5.2: Invoice-level behavioral features\n",
    "# ============================================================\n",
    "df_clean[\"InvoiceUniqueItems\"] = df_clean.groupby(\"Invoice\")[\"StockCode\"].transform(\"nunique\")\n",
    "df_clean[\"InvoiceTotalQuantity\"] = df_clean.groupby(\"Invoice\")[\"Quantity\"].transform(\"sum\")\n",
    "\n",
    "display(df_clean[[\"Invoice\",\"StockCode\",\"Quantity\",\"InvoiceUniqueItems\",\"InvoiceTotalQuantity\",\"LogTotalPrice\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0b598",
   "metadata": {},
   "source": [
    "## 6. Regression Modeling (Predict Transaction Value)<br>\n",
    "**Goal:** Predict **LogTotalPrice** (log-transformed revenue per transaction line) using transaction + time + basket features.<br><br>\n",
    "Models used:<br>\n",
    "• Linear Regression (baseline)<br>\n",
    "• Ridge Regression (regularized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f602c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6.1: Build feature matrix X and target y\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Target\n",
    "y_reg = df_clean[\"LogTotalPrice\"]\n",
    "\n",
    "# Features (drop leakage/unused fields)\n",
    "X_reg = df_clean.drop(columns=[\"TotalPrice\", \"LogTotalPrice\", \"InvoiceDate\"])\n",
    "\n",
    "# Remove free-text column (baseline regression)\n",
    "if \"Description\" in X_reg.columns:\n",
    "    X_reg = X_reg.drop(columns=[\"Description\"])\n",
    "\n",
    "# Ensure categorical columns are strings for OneHotEncoder\n",
    "num_cols = X_reg.select_dtypes(include=\"number\").columns.tolist()\n",
    "cat_cols = X_reg.select_dtypes(exclude=\"number\").columns.tolist()\n",
    "\n",
    "X_reg = X_reg.copy()\n",
    "for c in cat_cols:\n",
    "    X_reg[c] = X_reg[c].astype(str)\n",
    "\n",
    "print(\"Regression X shape:\", X_reg.shape)\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocessing: scale numeric + one-hot encode categorical\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Step 6.2: Train Linear Regression\n",
    "# ============================================================\n",
    "lr_model = Pipeline(steps=[(\"prep\", preprocess), (\"model\", LinearRegression())])\n",
    "lr_model.fit(X_train, y_train)\n",
    "pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "rmse_lr = mean_squared_error(y_test, pred_lr) ** 0.5\n",
    "r2_lr = r2_score(y_test, pred_lr)\n",
    "\n",
    "# ============================================================\n",
    "# Step 6.3: Train Ridge Regression\n",
    "# ============================================================\n",
    "ridge_model = Pipeline(steps=[(\"prep\", preprocess), (\"model\", Ridge(alpha=1.0))])\n",
    "ridge_model.fit(X_train, y_train)\n",
    "pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "rmse_ridge = mean_squared_error(y_test, pred_ridge) ** 0.5\n",
    "r2_ridge = r2_score(y_test, pred_ridge)\n",
    "\n",
    "# ============================================================\n",
    "# Step 6.4: Cross-validation (RMSE)\n",
    "# ============================================================\n",
    "cv_lr = cross_val_score(lr_model, X_reg, y_reg, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "cv_ridge = cross_val_score(ridge_model, X_reg, y_reg, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "\n",
    "cv_rmse_lr = (-cv_lr.mean()) ** 0.5\n",
    "cv_rmse_ridge = (-cv_ridge.mean()) ** 0.5\n",
    "\n",
    "results_reg = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Ridge Regression\"],\n",
    "    \"Test RMSE\": [rmse_lr, rmse_ridge],\n",
    "    \"Test R^2\": [r2_lr, r2_ridge],\n",
    "    \"CV RMSE\": [cv_rmse_lr, cv_rmse_ridge],\n",
    "})\n",
    "\n",
    "display(results_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46724fc1",
   "metadata": {},
   "source": [
    "## 7. Classification (High-Value Customer Prediction)<br>\n",
    "We aggregate transactions to the customer level (RFM-style features). High-value customers are defined as customers whose total spending is in the **top 25%**. We then train models to classify high-value customers based on behavioral features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 7.1: Customer-level feature construction (RFM-style)\n",
    "# ============================================================\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, f1_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Use a snapshot date for Recency calculation\n",
    "snapshot_date = df_clean[\"InvoiceDate\"].max() + pd.Timedelta(days=1)\n",
    "\n",
    "cust = df_clean.groupby(\"CustomerID\").agg(\n",
    "    Recency=(\"InvoiceDate\", lambda x: (snapshot_date - x.max()).days),\n",
    "    Frequency=(\"Invoice\", \"nunique\"),\n",
    "    Monetary=(\"TotalPrice\", \"sum\"),\n",
    "    AvgUnitPrice=(\"UnitPrice\", \"mean\"),\n",
    "    AvgQuantity=(\"Quantity\", \"mean\"),\n",
    "    UniqueProducts=(\"StockCode\", \"nunique\"),\n",
    ").reset_index()\n",
    "\n",
    "# Label: HighValue = top 25% by Monetary\n",
    "threshold = cust[\"Monetary\"].quantile(0.75)\n",
    "cust[\"HighValue\"] = (cust[\"Monetary\"] >= threshold).astype(int)\n",
    "\n",
    "print(\"HighValue distribution:\")\n",
    "display(cust[\"HighValue\"].value_counts())\n",
    "\n",
    "# IMPORTANT NOTE (avoids leakage):\n",
    "# Since HighValue is defined using Monetary, we exclude Monetary from predictors.\n",
    "features_cls = [\"Recency\", \"Frequency\", \"AvgUnitPrice\", \"AvgQuantity\", \"UniqueProducts\"]\n",
    "X_cls = cust[features_cls]\n",
    "y_cls = cust[\"HighValue\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.25, random_state=42, stratify=y_cls\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# ============================================================\n",
    "# Step 7.2: Logistic Regression baseline\n",
    "# ============================================================\n",
    "lr_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "lr_pred = lr_pipe.predict(X_test)\n",
    "lr_proba = lr_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"LogReg Accuracy:\", accuracy_score(y_test, lr_pred))\n",
    "print(\"LogReg F1:\", f1_score(y_test, lr_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, lr_pred)).plot()\n",
    "plt.title(\"Logistic Regression - Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, lr_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc(fpr,tpr):.3f}\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Logistic Regression - ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Step 7.3: Random Forest + hyperparameter tuning\n",
    "# ============================================================\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None, 6, 12],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "print(\"Best RF params:\", grid.best_params_)\n",
    "print(\"Best CV F1:\", grid.best_score_)\n",
    "\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "rf_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Tuned RF Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(\"Tuned RF F1:\", f1_score(y_test, rf_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, rf_pred)).plot()\n",
    "plt.title(\"Tuned Random Forest - Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, rf_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc(fpr,tpr):.3f}\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Tuned Random Forest - ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf926c",
   "metadata": {},
   "source": [
    "## 8. Clustering (Customer Segmentation)<br>\n",
    "K-Means clustering is used to segment customers based on RFM-style behavioral features. PCA is used for a 2D visualization of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb24991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 8.1: Prepare data for clustering\n",
    "# - To reduce the effect of extreme outliers, we clip each feature at the 1st/99th percentiles.\n",
    "# ============================================================\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "cluster_features = [\"Recency\", \"Frequency\", \"Monetary\", \"AvgUnitPrice\", \"AvgQuantity\", \"UniqueProducts\"]\n",
    "\n",
    "cust_cluster = cust.copy()\n",
    "for col in cluster_features:\n",
    "    low, high = cust_cluster[col].quantile([0.01, 0.99])\n",
    "    cust_cluster[col] = cust_cluster[col].clip(lower=low, upper=high)\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(cust_cluster[cluster_features])\n",
    "\n",
    "# ============================================================\n",
    "# Step 8.2: Fit KMeans (k=3 is a reasonable baseline; adjust if needed)\n",
    "# ============================================================\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cust_cluster[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(\"Cluster counts:\")\n",
    "display(cust_cluster[\"Cluster\"].value_counts())\n",
    "\n",
    "# ============================================================\n",
    "# Step 8.3: PCA visualization (2D)\n",
    "# ============================================================\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "coords = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x=coords[:,0], y=coords[:,1], hue=cust_cluster[\"Cluster\"], palette=\"tab10\")\n",
    "plt.title(\"Customer Clusters (K-Means) - PCA Visualization\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()\n",
    "\n",
    "# Cluster profile table\n",
    "cluster_profile = cust_cluster.groupby(\"Cluster\")[cluster_features + [\"HighValue\"]].mean().round(2)\n",
    "display(cluster_profile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f10175",
   "metadata": {},
   "source": [
    "## 9. Association Rule Mining (Market Basket Analysis)<br>\n",
    "We apply the Apriori algorithm to discover products that are frequently purchased together. To keep computation practical, we limit the analysis to **United Kingdom** transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024482f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 9.1: Build basket matrix (UK only)\n",
    "# ============================================================\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "df_rules = df_clean[df_clean[\"Country\"] == \"United Kingdom\"].copy()\n",
    "\n",
    "basket = (df_rules\n",
    "          .groupby([\"Invoice\", \"StockCode\"])[\"Quantity\"]\n",
    "          .sum()\n",
    "          .unstack(fill_value=0))\n",
    "\n",
    "# Convert counts to boolean (bought/not bought)\n",
    "basket = basket > 0\n",
    "\n",
    "print(\"Basket shape (invoices x products):\", basket.shape)\n",
    "\n",
    "# ============================================================\n",
    "# Step 9.2: Frequent itemsets\n",
    "# ============================================================\n",
    "freq_items = apriori(basket, min_support=0.01, use_colnames=True)\n",
    "print(\"Frequent itemsets:\", freq_items.shape[0])\n",
    "display(freq_items.sort_values(\"support\", ascending=False).head(10))\n",
    "\n",
    "# ============================================================\n",
    "# Step 9.3: Association rules (sorted by Lift)\n",
    "# ============================================================\n",
    "rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "rules = rules.sort_values([\"lift\", \"confidence\"], ascending=False)\n",
    "\n",
    "display(rules.head(10))\n",
    "\n",
    "# Quick visualization: top 10 rules by lift\n",
    "top_rules = rules.head(10).copy()\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.barplot(x=top_rules[\"lift\"], y=top_rules[\"antecedents\"].astype(str))\n",
    "plt.title(\"Top Association Rules by Lift (Antecedents)\")\n",
    "plt.xlabel(\"Lift\")\n",
    "plt.ylabel(\"Antecedent\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff43afb",
   "metadata": {},
   "source": [
    "## 10. Key Findings & Practical Recommendations<br>\n",
    "**Regression:** Ridge Regression slightly improved generalization and provides a stable approach for forecasting transaction value.<br><br>\n",
    "**Classification:** Behavioral features (recency, frequency, pricing patterns, product diversity) help identify high-value customers for retention and targeted marketing.<br><br>\n",
    "**Clustering:** Customer segments enable differentiated strategies such as loyalty rewards for frequent buyers and win-back offers for at-risk customers.<br><br>\n",
    "**Association Rules:** Frequently co-purchased items support bundling, cross-selling, and recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84defb",
   "metadata": {},
   "source": [
    "## 11. Ethical Considerations<br>\n",
    "This project uses customer identifiers (CustomerID) that are not direct personal identifiers, but ethical handling is still required.<br><br>\n",
    "**Privacy:** Analysis is performed on anonymized IDs and aggregated patterns. No attempt is made to re-identify individuals.<br>\n",
    "**Bias & representativeness:** Sales are heavily concentrated in the United Kingdom; insights may not generalize to other regions or time periods.<br>\n",
    "**Fairness:** Predictive models should not be used to unfairly exclude customers from services. Any business action should include human oversight and transparency.<br>\n",
    "**Mitigation steps:** Aggregation at customer level, excluding sensitive attributes, and cautious interpretation of model outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455185c",
   "metadata": {},
   "source": [
    "## 12. Conclusion & Future Improvements<br>\n",
    "By combining EDA, regression, classification, clustering, and association mining, this project demonstrates how big data techniques produce actionable retail insights.<br><br>\n",
    "**Future improvements:** incorporate richer text features from product descriptions, test additional models (e.g., gradient boosting), and evaluate stability across time (train on earlier months, test on later months).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
